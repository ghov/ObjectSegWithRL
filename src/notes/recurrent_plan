state = a vector of 1 by n, where n is 2*number of vertices.
reward = the value of taking an action
action = a movement of a vertex in the predefined direction by a predefined amount.
action-reward vector = a vector where each value is the reward for taking that action, given a certain state.
    = we assume that the action with the highest reward will be chosen, given a state.




label = These are produced at run time. Given a state, take each action and get the reward after taking that action.
    = The reward is a vector of size 1 by actions.



FLOW:
1. An initial state is generated, in this case a polygon. This can be random or can include the entire image at first.
2. Feed the image into a cnn, which generates a vector of features.
3. Concatenate these features with the t-1 state vector.
4. This is used as input for the estimator network, which produces a (1 by number of actions) vector as output, where
each value in the vector is the estimated reward of taking that action, given the current state.
5. To generate a dynamic label, take the current state and apply each possible action to that state. Then, take the
new iou minus the old iou and return that as the reward.
6. Use regular loss, like mse to compare the dynamic label with the output of the evaluator network. Use this to
backpropogate and learn
7. Take softmax of the estimated reward matrix and get the index of the largest value. Get the action associated with
this index and apply it to the current state. Save this as the new state.
8. Also compute the IoU of this new state with the ground truth.
9. Stop training after reaching some predefined number of max steps or until a stop action is chosen.
10. We need to get to a point where every predicted reward is negative, except for the stop action.
Or every reward is smaller than the stop action.



# Notes:
1. It looks like coco can compute iou from segmentation poly, without needing to convert to mask. But may need
segmentations of the same size.
2. Need to make a version of annToRLE from coco.py that takes as input the height and width.
3. Might be better to use softmax at the final stage, instead of just the regular rewards.


NEXT TO DO:
RUN FOR MORE THAN ONE IMAGE.


# DONE - # NEED TO MAKE SURE THAT NONE OF THE CO-ORDINATES EVER GO BELOW 0. MAYBE PROVIDE BIG NEGATIVE REWARD FOR THIS.
# DONE - # NEED TO MAKE SURE THE COORDINATES NEVER GO ABOVE THE HEIGHT OR WIDTH
# DONE - # For some reason, choosing to decrease a 0 has a high reward. Investigate!!!!
# DONE -     # THERE IS A LARGE POSITIVE REWARD FOR CHOOSING TO STAY WITH THE CURRENT POLYGON !!!
# DONE -     # The problem is get_changed_polygons_from_polygon. It doesn't have the max or min limit.



08/09/2018:
Thoughts on progress so far.
1. I'm currently training the entire network. This means that at each time step, the polygon vector is not the only
change. The vector representing the image features is also different. Hence, two polygons with the same image can be
seen as two different states by the model, making it difficult to learn the reward for each polygon state. It might be
useful to keep the image feature vector constant. This way, each state differs only by the polygon vector. Hence, it
should be easier for the model to learn appropriate weights to get the correct reward estimate vector.
    a. This can be done by having two separate models, one for getting the features of the image and another for getting
    the predicted reward vector. The image features can be done by getting the imagenet trained version of the models
    that I am using.
    b. Since this system requires two models, it is no longer an end to end fully trained system.
    c. We will also only need to run the image feature cnn portion only once for each image.
    d. One way is to just call the model.features() function, then .view(1, other dim).

2. Since this system trains one action at a time, it might be more appropriate to use a recurrent model, such as an
lstm. However, since our model is not fully differentiable, this might be difficult.

3. It would be interesting to see the difference between an alexnet based system and a vgg based system. In other words,
make all the training parameters the same, except for the model.Though, in the single model system, the results might be
 the same or very similar, i.e. both having terrible results.
    a. Did this for the end-to-end model with   "max_steps" : 150, "epochs" : 50, and found no significant difference
    between the two.

4. It is time to get organized again. There are function in the resize_functions.py function that have nothing to do
with resizing. They should be moved somewhere else, maybe into helper_functions.py.
    a. This is done

5. It seems that there exists an important relationship between epochs and steps. The epochs should always be
significantly larger than the number of steps. In fact, I have found that for each step, I need about 50+ steps. This
was tested on the two model system. This works with a small number of steps, such as 10. However, with more steps it
seems that the learning fails anyway. Not sure what is going on here.

6. I suspect that there exists an important relationship between the size of the image features vector and the state
vector. The change between each step is very small, as only one scalar in the entire combined vector changes. The image
feature vectors are significantly larger than the polygon state vector. It may be useful to have a smaller
representation of the image features.
    a. Furthermore, it looks like the model tends to average the reward vector across similar inputs. And since the
    inputs have a very small difference at each step, then the model doesn't form distinct weights per state.

10. Currently, there is only memory of the current state. It might be useful to remember more states.
    a. Update, this does not seem to make a positive difference. It might be making a negative difference, in fact.

11. Read that in Deep Q Network, they used experience replay to greatly increase the results. Also used a second fixed
Q network to further increase results. Could implement experience replay and see if that fixes the problem. This
involves saving the (state, action, reward, new state) in a big table. Later on, instead of taking a normal step, we
sample from this big table and learn from that.

12. Should consider adding random choices to the model. Instead of choosing the largest return prediction during
training, might be better to always take a random move or simply take a random move with alpha probability. This means
the model would not be simply seeing the same results over and over again.

    a. Added training code that only makes random decisions. Interestingly, the model seems to be more consistent with
    regards to loss. In testing, however, the model chooses one action and continuously applies that action. No other
    action is ever chosen. Same problem as seen in the past.

13. Should consider branching in git. Need to explore multiple possible ways of improving the model, which will
require significant modification of the existing code. Otherwise could make new file and store the new code there.
    a. This is done.

7. Just did a train with these configurations:   "reward_multiplier" : 100,
  "step_cost" : -0.0005,
  "coordinate_action_change_amount" : 10,
  "number_of_actions" : 17,
  "polygon_state_length" : 8,
  "height_initial" : 224,
  "width_initial" : 224,
  "max_steps" : 30,
  "epochs" : 3000,
  "model" : "vgg19_bn",
  "stop_action_reward" : 0,
  "learning_rate" : 0.001,
  "weight_decay" : 0.0001,
  "batch_size" : 1,
  "num_workers" : 4,
  "file_paths" : {
    "image_dir" : "/media/greghovhannisyan/BackupData1/mscoco/images/by_vertex/temp_1/",
    "polygon_path" : "/media/greghovhannisyan/BackupData1/mscoco/annotations/by_vertex/temp1.json",
    "model_dir" : "/home/greghovhannisyan/PycharmProjects/towards_rlnn_cnn/ObjectSegWithRL/data/models/cnn/",
    "model_name" : "vgg19_bn_pre"

  a. The results were actually quite good. It modified multiple scalars throughout the steps, not just focusing on one
  value. Also, it did not keep making illegal moves. Might be because there are 100 epochs for each step. Or perhaps
  at 30 steps, it is still able to learn well.

8. Increasing the step cost gives the same result, where we the model keeps choosing a single point to change.

9. The biggest issue is that the system learns to average things. Hence the weights just try to average, not make
decision based on each state.

14. Figured out how to use opencv2 to create an image where everything is black, except the area that the mask covers.
Can use this in the model, since this avoids having to attach a polygon vector to the image features vector. This should
 help with the problem of the states being too similar, hence possibly causing averaging of the output. However, it may
 be a bad idea to black out everything that isn't in the mask. Might consider putting a highlight around the mask,
 instead of blacking things out.









