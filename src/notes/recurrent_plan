state = a vector of 1 by n, where n is 2*number of vertices.
reward = the value of taking an action
action = a movement of a vertex in the predefined direction by a predefined amount.
action-reward vector = a vector where each value is the reward for taking that action, given a certain state.
    = we assume that the action with the highest reward will be chosen, given a state.




label = These are produced at run time. Given a state, take each action and get the reward after taking that action.
    = The reward is a vector of size 1 by actions.



FLOW:
1. An initial state is generated, in this case a polygon. This can be random or can include the entire image at first.
2. Feed the image into a cnn, which generates a vector of features.
3. Concatenate these features with the t-1 state vector.
4. This is used as input for the estimator network, which produces a (1 by number of actions) vector as output, where
each value in the vector is the estimated reward of taking that action, given the current state.
5. To generate a dynamic label, take the current state and apply each possible action to that state. Then, take the
new iou minus the old iou and return that as the reward.
6. Use regular loss, like mse to compare the dynamic label with the output of the evaluator network. Use this to
backpropogate and learn
7. Take softmax of the estimated reward matrix and get the index of the largest value. Get the action associated with
this index and apply it to the current state. Save this as the new state.
8. Also compute the IoU of this new state with the ground truth.
9. Stop training after reaching some predefined number of max steps or until a stop action is chosen.
10. We need to get to a point where every predicted reward is negative, except for the stop action.
Or every reward is smaller than the stop action.



# Notes:
1. It looks like coco can compute iou from segmentation poly, without needing to convert to mask. But may need
segmentations of the same size.
2. Need to make a version of annToRLE from coco.py that takes as input the height and width.
3. Might be better to use softmax at the final stage, instead of just the regular rewards.


NEXT TO DO:
RUN FOR MORE THAN ONE IMAGE.


# DONE - # NEED TO MAKE SURE THAT NONE OF THE CO-ORDINATES EVER GO BELOW 0. MAYBE PROVIDE BIG NEGATIVE REWARD FOR THIS.
# DONE - # NEED TO MAKE SURE THE COORDINATES NEVER GO ABOVE THE HEIGHT OR WIDTH
# DONE - # For some reason, choosing to decrease a 0 has a high reward. Investigate!!!!
# DONE -     # THERE IS A LARGE POSITIVE REWARD FOR CHOOSING TO STAY WITH THE CURRENT POLYGON !!!
# DONE -     # The problem is get_changed_polygons_from_polygon. It doesn't have the max or min limit.



08/09/2018:
Thoughts on progress so far.
1. I'm currently training the entire network. This means that at each time step, the polygon vector is not the only
change. The vector representing the image features is also different. Hence, two polygons with the same image can be
seen as two different states by the model, making it difficult to learn the reward for each polygon state. It might be
useful to keep the image feature vector constant. This way, each state differs only by the polygon vector. Hence, it
should be easier for the model to learn appropriate weights to get the correct reward estimate vector.
    a. This can be done by having two separate models, one for getting the features of the image and another for getting
    the predicted reward vector. The image features can be done by getting the imagenet trained version of the models
    that I am using.
    b. Since this system requires two models, it is no longer an end to end fully trained system.
    c. We will also only need to run the image feature cnn portion only once for each image.
    d. One way is to just call the model.features() function, then .view(1, other dim).

2. Since this system trains one action at a time, it might be more appropriate to use a recurrent model, such as an
lstm. However, since our model is not fully differentiable, this might be difficult.

3. It would be interesting to see the difference between an alexnet based system and a vgg based system. In other words,
make all the training parameters the same, except for the model.Though, in the single model system, the results might be
 the same or very similar, i.e. both having terrible results.
    a. Did this for the end-to-end model with   "max_steps" : 150, "epochs" : 50, and found no significant difference
    between the two.

4. It is time to get organized again. There are function in the resize_functions.py function that have nothing to do
with resizing. They should be moved somewhere else, maybe into helper_functions.py.
    a. This is done







